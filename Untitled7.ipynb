{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayBista1010/my-first-repo/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxC8V-oVS0Fc",
        "outputId": "d65fcc45-52f3-4439-ab61-3b20f17563ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Extracted files to /content/images\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to your ZIP file in Google Drive\n",
        "zip_path = '/content/drive/MyDrive/dataset.zip'\n",
        "\n",
        "# Destination folder in Colab\n",
        "extract_path = '/content/images'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Extracted files to {extract_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7gwKdvJSitP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002436f4-df1e-40b3-866c-832910874987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected classes: ['snake', 'spider']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:17<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 ‚Üí Train Loss: 0.7508 | Train Acc: 58.58% | Val Acc: 69.63% | LR: 0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:17<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 ‚Üí Train Loss: 0.6282 | Train Acc: 67.15% | Val Acc: 72.56% | LR: 0.000098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:23<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 ‚Üí Train Loss: 0.6120 | Train Acc: 69.13% | Val Acc: 74.82% | LR: 0.000091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:22<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4 ‚Üí Train Loss: 0.5914 | Train Acc: 71.51% | Val Acc: 76.50% | LR: 0.000080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:20<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5 ‚Üí Train Loss: 0.5830 | Train Acc: 72.50% | Val Acc: 75.93% | LR: 0.000066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:19<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6 ‚Üí Train Loss: 0.5705 | Train Acc: 73.84% | Val Acc: 79.09% | LR: 0.000051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:17<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7 ‚Üí Train Loss: 0.5568 | Train Acc: 75.38% | Val Acc: 79.85% | LR: 0.000035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:20<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8 ‚Üí Train Loss: 0.5337 | Train Acc: 77.37% | Val Acc: 81.11% | LR: 0.000021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:20<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9 ‚Üí Train Loss: 0.5293 | Train Acc: 77.88% | Val Acc: 81.73% | LR: 0.000011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:17<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 ‚Üí Train Loss: 0.5215 | Train Acc: 78.50% | Val Acc: 81.98% | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:21<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 ‚Üí Train Loss: 0.5221 | Train Acc: 78.67% | Val Acc: 81.67% | LR: 0.000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:19<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 ‚Üí Train Loss: 0.5605 | Train Acc: 75.12% | Val Acc: 79.83% | LR: 0.000099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158/158 [02:17<00:00,  1.15it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Custom VGG16 Model\n",
        "# -----------------------------\n",
        "class CustomVGG16(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=2):\n",
        "        super(CustomVGG16, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(in_channels, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            # Block 2\n",
        "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            # Block 3\n",
        "            nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            # Block 4\n",
        "            nn.Conv2d(256,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            # Block 5\n",
        "            nn.Conv2d(512,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512,512,3,padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512*7*7, 4096), nn.ReLU(inplace=True), nn.Dropout(0.6),\n",
        "            nn.Linear(4096, 1024), nn.ReLU(inplace=True), nn.Dropout(0.6),\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Hyperparameters\n",
        "# -----------------------------\n",
        "batch_size = 128\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 50\n",
        "accum_steps = 2  # Gradient accumulation steps\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Transforms\n",
        "# -----------------------------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.85,1.0)),\n",
        "    transforms.RandomErasing(p=0.3, scale=(0.02,0.2), ratio=(0.3,3.3), value='random'),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Dataset & DataLoader\n",
        "# -----------------------------\n",
        "dataset_path = \"images/\"\n",
        "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform_train)\n",
        "\n",
        "num_classes = len(full_dataset.classes)\n",
        "print(\"Detected classes:\", full_dataset.classes)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "val_dataset.dataset.transform = transform_val\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Device & Model\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomVGG16(num_classes=num_classes).to(device)\n",
        "\n",
        "# Freeze first 3 blocks initially\n",
        "for param in list(model.features[:16].parameters()):\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Optimizer + LR Scheduler (CosineAnnealingWarmRestarts)\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                        lr=learning_rate, weight_decay=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
        ")\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = torch.amp.GradScaler(device=device)\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ MixUp functions\n",
        "# -----------------------------\n",
        "def mixup_data(x, y, alpha=0.4):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Class Weights + Loss\n",
        "# -----------------------------\n",
        "targets = [label for _, label in full_dataset.samples]\n",
        "class_counts = np.bincount(targets)\n",
        "class_weights = 1. / class_counts\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Training Loop\n",
        "# -----------------------------\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "patience = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    for i, (imgs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        # MixUp\n",
        "        imgs, targets_a, targets_b, lam = mixup_data(imgs, labels)\n",
        "\n",
        "        # Mixed Precision\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            outputs = model(imgs)\n",
        "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam) / accum_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i+1) % accum_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item() * accum_steps  # scale back\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += (lam * (preds==targets_a).sum() + (1-lam)*(preds==targets_b).sum()).item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct_val, total_val = 0,0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct_val += (preds==labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} ‚Üí Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | LR: {current_lr:.6f}\")\n",
        "\n",
        "    scheduler.step(epoch + i/len(train_loader))  # Warm restarts with fractional epoch\n",
        "\n",
        "    # üîì Unfreeze all blocks at epoch 15\n",
        "    if epoch == 15:\n",
        "        print(\"\\nüîì Unfreezing all layers for fine-tuning...\")\n",
        "        for param in model.features.parameters():\n",
        "            param.requires_grad = True\n",
        "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5, weight_decay=1e-3)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve +=1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "# -----------------------------\n",
        "# 9Ô∏è‚É£ Final Evaluation\n",
        "# -----------------------------\n",
        "all_preds, all_labels = [], []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNmyZNyk+Y2uKz+vVbH+BMi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}